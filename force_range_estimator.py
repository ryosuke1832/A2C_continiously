import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.calibration import CalibratedClassifierCV
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
import matplotlib
import platform

def setup_japanese_font():
    system = platform.system()
    try:
        if system == "Windows":
            matplotlib.rcParams['font.family'] = ['Yu Gothic', 'Meiryo']
        elif system == "Darwin":
            matplotlib.rcParams['font.family'] = ['Hiragino Sans', 'Yu Gothic']
        else:
            matplotlib.rcParams['font.family'] = ['Takao', 'IPAexGothic', 'DejaVu Sans']
        matplotlib.rcParams['axes.unicode_minus'] = False
        return True
    except:
        return False

JAPANESE_AVAILABLE = setup_japanese_font()

class AIForceBoundaryClassifier:
    """
    AIæ‰‹æ³•ã«ã‚ˆã‚‹åŠ›å¢ƒç•Œåˆ†é¡ã‚·ã‚¹ãƒ†ãƒ 
    è«–æ–‡ã®EEGNetã‚„æ·±å±¤å­¦ç¿’æ‰‹æ³•ã‚’åŠ›åˆ†é¡ã«å¿œç”¨
    """
    
    def __init__(self, csv_file_path):
        self.csv_file_path = csv_file_path
        self.data = None
        self.models = {}
        self.scalers = {}
        self.best_model = None
        self.feature_importance = None
        
    def load_and_preprocess_data(self):
        """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"""
        try:
            self.data = pd.read_csv(self.csv_file_path)
            print(f"ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(self.data)} è¡Œ")
            
            # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
            self.data = self.create_features(self.data)
            
            print(f"ç‰¹å¾´é‡æ•°: {len(self.get_feature_columns())}")
            print(f"ã¤ã¶ã‚ŒãŸäº‹ä¾‹: {len(self.data[self.data['is_crushed']])} ä»¶")
            print(f"å®‰å…¨äº‹ä¾‹: {len(self.data[~self.data['is_crushed']])} ä»¶")
            
            return True
        except Exception as e:
            print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def create_features(self, df):
        """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° - è«–æ–‡ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«åŸºã¥ã"""
        df = df.copy()
        
        # åŸºæœ¬çš„ãªçµ±è¨ˆçš„ç‰¹å¾´é‡
        df['force_squared'] = df['current_force'] ** 2
        df['force_log'] = np.log1p(df['current_force'])
        df['force_sqrt'] = np.sqrt(df['current_force'])
        
        # æ™‚ç³»åˆ—ç‰¹å¾´é‡ï¼ˆè«–æ–‡ã®æ™‚ç³»åˆ—è§£æã«åŸºã¥ãï¼‰
        window_sizes = [3, 5, 10]
        for window in window_sizes:
            # ç§»å‹•å¹³å‡
            df[f'force_ma_{window}'] = df['current_force'].rolling(window=window, min_periods=1).mean()
            # ç§»å‹•æ¨™æº–åå·®
            df[f'force_std_{window}'] = df['current_force'].rolling(window=window, min_periods=1).std().fillna(0)
            # åŠ›ã®å¤‰åŒ–ç‡
            df[f'force_change_{window}'] = df['current_force'].diff(window).fillna(0)
        
        # ç´¯ç©çµ±è¨ˆé‡
        df['force_cumsum'] = df['current_force'].cumsum()
        df['force_cummax'] = df['current_force'].cummax()
        df['force_cummin'] = df['current_force'].cummin()
        
        # å±€æ‰€çš„ãªæ¥µå€¤æ¤œå‡º
        df['force_local_max'] = df['current_force'].rolling(5, center=True).max() == df['current_force']
        df['force_local_min'] = df['current_force'].rolling(5, center=True).min() == df['current_force']
        
        # åŠ›ã®å®‰å®šæ€§æŒ‡æ¨™
        df['force_stability'] = 1.0 / (1.0 + df['force_std_5'])
        
        # ã‚¹ãƒ†ãƒƒãƒ—æƒ…å ±ã‚’æ™‚é–“çš„ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ç”¨ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
        if 'step' in df.columns:
            max_step = df['step'].max() if df['step'].max() > 0 else 1
            df['step_normalized'] = df['step'] / max_step
        else:
            df['step_normalized'] = 0
        
        return df
    
    def get_feature_columns(self):
        """ç‰¹å¾´é‡åˆ—åã‚’å–å¾—"""
        exclude_cols = ['episode', 'step', 'is_crushed', 'predicted_category', 
                       'recommended_force', 'confidence', 'timestamp', 'accumulated_force']
        return [col for col in self.data.columns if col not in exclude_cols]
    
    def build_deep_neural_network(self, input_dim):
        """è«–æ–‡ã®EEGNetã«ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ã•ã‚ŒãŸæ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
        model = Sequential([
            # å…¥åŠ›å±¤
            Dense(128, activation='relu', input_shape=(input_dim,)),
            BatchNormalization(),
            Dropout(0.3),
            
            # éš ã‚Œå±¤1ï¼ˆè«–æ–‡ã®32ãƒ¦ãƒ‹ãƒƒãƒˆã‚’æ‹¡å¼µï¼‰
            Dense(64, activation='relu'),
            BatchNormalization(),
            Dropout(0.3),
            
            # éš ã‚Œå±¤2
            Dense(32, activation='relu'),
            BatchNormalization(),
            Dropout(0.2),
            
            # éš ã‚Œå±¤3
            Dense(16, activation='relu'),
            Dropout(0.2),
            
            # å‡ºåŠ›å±¤ï¼ˆãƒã‚¤ãƒŠãƒªåˆ†é¡ï¼‰
            Dense(1, activation='sigmoid')
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        return model
    
    def train_all_models(self):
        """è¤‡æ•°ã®AIãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
        if self.data is None:
            print("å…ˆã«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„")
            return
        
        # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæº–å‚™
        feature_cols = self.get_feature_columns()
        X = self.data[feature_cols].fillna(0).values
        y = self.data['is_crushed'].values.astype(int)
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        self.scalers['main'] = scaler
        
        # 1. Random Forestï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ï¼‰
        print("ğŸŒ² Random Forest ã‚’è¨“ç·´ä¸­...")
        rf_params = {
            'n_estimators': [100, 200, 300],
            'max_depth': [10, 20, None],
            'min_samples_split': [2, 5, 10]
        }
        rf = RandomForestClassifier(random_state=42)
        rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring='roc_auc', n_jobs=-1)
        rf_grid.fit(X_train, y_train)
        self.models['random_forest'] = rf_grid.best_estimator_
        
        # 2. XGBoostï¼ˆå‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼‰
        print("ğŸš€ XGBoost ã‚’è¨“ç·´ä¸­...")
        xgb_params = {
            'n_estimators': [100, 200],
            'max_depth': [3, 6, 9],
            'learning_rate': [0.01, 0.1, 0.2]
        }
        xgb_model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')
        xgb_grid = GridSearchCV(xgb_model, xgb_params, cv=5, scoring='roc_auc', n_jobs=-1)
        xgb_grid.fit(X_train, y_train)
        self.models['xgboost'] = xgb_grid.best_estimator_
        
        # 3. Support Vector Machine
        print("âš¡ SVM ã‚’è¨“ç·´ä¸­...")
        svm_params = {
            'C': [0.1, 1, 10],
            'gamma': ['scale', 'auto', 0.1, 1],
            'kernel': ['rbf', 'linear']
        }
        svm = SVC(probability=True, random_state=42)
        svm_grid = GridSearchCV(svm, svm_params, cv=5, scoring='roc_auc', n_jobs=-1)
        svm_grid.fit(X_train_scaled, y_train)
        self.models['svm'] = svm_grid.best_estimator_
        
        # 4. æ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆè«–æ–‡ã®EEGNetã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
        print("ğŸ§  æ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ ã‚’è¨“ç·´ä¸­...")
        dnn = self.build_deep_neural_network(X_train_scaled.shape[1])
        
        early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)
        
        history = dnn.fit(
            X_train_scaled, y_train,
            epochs=200,
            batch_size=32,
            validation_split=0.2,
            callbacks=[early_stopping, reduce_lr],
            verbose=0
        )
        
        self.models['deep_neural_network'] = dnn
        self.training_history = history
        
        # 5. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ï¼ˆè«–æ–‡ã®æ‰‹æ³•çµ±åˆï¼‰
        print("ğŸ¯ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ« ã‚’æ§‹ç¯‰ä¸­...")
        ensemble_models = [
            ('rf', self.models['random_forest']),
            ('xgb', self.models['xgboost']),
            ('svm', self.models['svm'])
        ]
        
        # Voting Classifier
        from sklearn.ensemble import VotingClassifier
        ensemble = VotingClassifier(
            estimators=ensemble_models,
            voting='soft'
        )
        ensemble.fit(X_train_scaled, y_train)
        self.models['ensemble'] = ensemble
        
        # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
        self.X_test = X_test_scaled
        self.y_test = y_test
        self.X_train = X_train_scaled
        self.y_train = y_train
        
        self.evaluate_all_models()
        self.select_best_model()
        
    def evaluate_all_models(self):
        """å…¨ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡"""
        print("\n" + "="*60)
        print("           ğŸ† AI ãƒ¢ãƒ‡ãƒ«æ€§èƒ½è©•ä¾¡")
        print("="*60)
        
        model_scores = {}
        
        for name, model in self.models.items():
            if name == 'deep_neural_network':
                y_pred_proba = model.predict(self.X_test).flatten()
                y_pred = (y_pred_proba > 0.5).astype(int)
            else:
                y_pred_proba = model.predict_proba(self.X_test)[:, 1]
                y_pred = model.predict(self.X_test)
            
            # å„ç¨®ã‚¹ã‚³ã‚¢è¨ˆç®—
            auc_score = roc_auc_score(self.y_test, y_pred_proba)
            accuracy = (y_pred == self.y_test).mean()
            
            # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢
            if name != 'deep_neural_network':
                cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=5, scoring='roc_auc')
                cv_mean = cv_scores.mean()
                cv_std = cv_scores.std()
            else:
                cv_mean = cv_std = 0  # DNNã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¯æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—
            
            model_scores[name] = {
                'auc': auc_score,
                'accuracy': accuracy,
                'cv_mean': cv_mean,
                'cv_std': cv_std,
                'predictions': y_pred_proba
            }
            
            print(f"\nğŸ“Š {name.upper()}:")
            print(f"  - AUC Score: {auc_score:.4f}")
            print(f"  - Accuracy: {accuracy:.4f}")
            if cv_mean > 0:
                print(f"  - CV Score: {cv_mean:.4f} (Â±{cv_std:.4f})")
        
        self.model_scores = model_scores
        
    def select_best_model(self):
        """æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ"""
        best_auc = 0
        best_name = None
        
        for name, scores in self.model_scores.items():
            if scores['auc'] > best_auc:
                best_auc = scores['auc']
                best_name = name
        
        self.best_model = self.models[best_name]
        print(f"\nğŸ¥‡ æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_name.upper()} (AUC: {best_auc:.4f})")
        
        # ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—
        if hasattr(self.best_model, 'feature_importances_'):
            feature_cols = self.get_feature_columns()
            self.feature_importance = pd.DataFrame({
                'feature': feature_cols,
                'importance': self.best_model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            print("\nğŸ“ˆ é‡è¦ãªç‰¹å¾´é‡ Top 5:")
            for idx, row in self.feature_importance.head().iterrows():
                print(f"  {row['feature']}: {row['importance']:.4f}")
    
    def predict_crush_probability(self, force_value, additional_features=None):
        """æŒ‡å®šã—ãŸåŠ›ã§ã¤ã¶ã‚Œã‚‹ç¢ºç‡ã‚’äºˆæ¸¬ï¼ˆAIãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼‰"""
        if self.best_model is None:
            print("å…ˆã«ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¦ãã ã•ã„")
            return None
        
        try:
            # åŸºæœ¬ç‰¹å¾´é‡ã‚’ä½œæˆ
            temp_df = pd.DataFrame({
                'current_force': [force_value],
                'step': [0],  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿½åŠ 
                'accumulated_force': [0]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿½åŠ 
            })
            temp_df = self.create_features(temp_df)
            
            # è¿½åŠ ç‰¹å¾´é‡ãŒã‚ã‚Œã°çµ±åˆ
            if additional_features:
                for key, value in additional_features.items():
                    temp_df[key] = value
            
            # ç‰¹å¾´é‡ã‚’æº–å‚™
            feature_cols = self.get_feature_columns()
            
            # å­˜åœ¨ã—ãªã„ç‰¹å¾´é‡ã¯0ã§åŸ‹ã‚ã‚‹
            for col in feature_cols:
                if col not in temp_df.columns:
                    temp_df[col] = 0
            
            X = temp_df[feature_cols].fillna(0).values
            X_scaled = self.scalers['main'].transform(X)
            
            # äºˆæ¸¬
            if hasattr(self.best_model, 'predict_proba'):
                prob = self.best_model.predict_proba(X_scaled)[0][1]
            else:  # Deep Neural Network
                prob = self.best_model.predict(X_scaled)[0][0]
            
            return float(prob)
            
        except Exception as e:
            print(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
    
    def find_optimal_force_range(self, target_crush_prob=0.05, force_range=None):
        """AIäºˆæ¸¬ã«åŸºã¥ãæœ€é©åŠ›ç¯„å›²ã®æ±ºå®š"""
        if force_range is None:
            min_force = self.data['current_force'].min()
            max_force = self.data['current_force'].max()
            force_range = np.linspace(min_force, max_force, 200)
        
        probabilities = [self.predict_crush_probability(f) for f in force_range]
        
        # ç›®æ¨™ç¢ºç‡ä»¥ä¸‹ã®ç¯„å›²ã‚’è¦‹ã¤ã‘ã‚‹
        safe_indices = np.where(np.array(probabilities) <= target_crush_prob)[0]
        
        if len(safe_indices) == 0:
            print(f"âš ï¸  ç›®æ¨™ç¢ºç‡ {target_crush_prob} ä»¥ä¸‹ã®å®‰å…¨ãªåŠ›ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return None, None
        
        safe_min = force_range[safe_indices[0]]
        safe_max = force_range[safe_indices[-1]]
        
        # æœ€é©ç¯„å›²ï¼ˆå®‰å…¨ç¯„å›²ã®ä¸­å¤®50%ï¼‰
        optimal_min = safe_min + (safe_max - safe_min) * 0.25
        optimal_max = safe_min + (safe_max - safe_min) * 0.75
        
        print(f"\nğŸ¯ AIäºˆæ¸¬ã«ã‚ˆã‚‹æœ€é©åŠ›ç¯„å›²:")
        print(f"  å®‰å…¨ç¯„å›²: {safe_min:.3f} - {safe_max:.3f} N (ã¤ã¶ã‚Œã‚‹ç¢ºç‡ â‰¤ {target_crush_prob})")
        print(f"  æœ€é©ç¯„å›²: {optimal_min:.3f} - {optimal_max:.3f} N")
        
        return (safe_min, safe_max), (optimal_min, optimal_max)
    
    def visualize_ai_results(self):
        """AIåˆ†æçµæœã®å¯è¦–åŒ–"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        labels = {
            'title': 'AIæ‰‹æ³•ã«ã‚ˆã‚‹åŠ›å¢ƒç•Œåˆ†æ' if JAPANESE_AVAILABLE else 'AI-based Force Boundary Analysis',
            'force': 'åŠ› (N)' if JAPANESE_AVAILABLE else 'Force (N)',
            'probability': 'ã¤ã¶ã‚Œã‚‹ç¢ºç‡' if JAPANESE_AVAILABLE else 'Crush Probability',
            'model_comparison': 'ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ' if JAPANESE_AVAILABLE else 'Model Performance Comparison',
            'roc_curves': 'ROCæ›²ç·š' if JAPANESE_AVAILABLE else 'ROC Curves',
            'feature_importance': 'ç‰¹å¾´é‡é‡è¦åº¦' if JAPANESE_AVAILABLE else 'Feature Importance',
            'prediction_surface': 'äºˆæ¸¬é¢' if JAPANESE_AVAILABLE else 'Prediction Surface',
            'training_history': 'è¨“ç·´å±¥æ­´' if JAPANESE_AVAILABLE else 'Training History'
        }
        
        # 1. ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ
        model_names = list(self.model_scores.keys())
        auc_scores = [self.model_scores[name]['auc'] for name in model_names]
        
        axes[0,0].bar(model_names, auc_scores, color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'lightpink'])
        axes[0,0].set_title(labels['model_comparison'])
        axes[0,0].set_ylabel('AUC Score')
        axes[0,0].tick_params(axis='x', rotation=45)
        axes[0,0].grid(True, alpha=0.3)
        
        # 2. ROCæ›²ç·š
        for name in model_names:
            if name in self.model_scores:
                fpr, tpr, _ = roc_curve(self.y_test, self.model_scores[name]['predictions'])
                auc = self.model_scores[name]['auc']
                axes[0,1].plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})')
        
        axes[0,1].plot([0, 1], [0, 1], 'k--', label='Random')
        axes[0,1].set_xlabel('False Positive Rate')
        axes[0,1].set_ylabel('True Positive Rate')
        axes[0,1].set_title(labels['roc_curves'])
        axes[0,1].legend()
        axes[0,1].grid(True, alpha=0.3)
        
        # 3. ç‰¹å¾´é‡é‡è¦åº¦
        if self.feature_importance is not None:
            top_features = self.feature_importance.head(10)
            axes[0,2].barh(range(len(top_features)), top_features['importance'])
            axes[0,2].set_yticks(range(len(top_features)))
            axes[0,2].set_yticklabels(top_features['feature'])
            axes[0,2].set_title(labels['feature_importance'])
            axes[0,2].grid(True, alpha=0.3)
        
        # 4. åŠ› vs ã¤ã¶ã‚Œã‚‹ç¢ºç‡
        force_range = np.linspace(self.data['current_force'].min(), 
                                self.data['current_force'].max(), 100)
        probabilities = [self.predict_crush_probability(f) for f in force_range]
        
        axes[1,0].plot(force_range, probabilities, 'b-', linewidth=2, label='AIäºˆæ¸¬')
        axes[1,0].axhline(y=0.05, color='r', linestyle='--', label='5%ãƒªã‚¹ã‚¯ãƒ©ã‚¤ãƒ³')
        axes[1,0].axhline(y=0.1, color='orange', linestyle='--', label='10%ãƒªã‚¹ã‚¯ãƒ©ã‚¤ãƒ³')
        axes[1,0].set_xlabel(labels['force'])
        axes[1,0].set_ylabel(labels['probability'])
        axes[1,0].set_title(labels['prediction_surface'])
        axes[1,0].legend()
        axes[1,0].grid(True, alpha=0.3)
        
        # 5. å®Ÿãƒ‡ãƒ¼ã‚¿ã¨ã®æ¯”è¼ƒ
        safe_data = self.data[self.data['is_crushed'] == False]
        crushed_data = self.data[self.data['is_crushed'] == True]
        
        axes[1,1].scatter(safe_data['current_force'], [0]*len(safe_data), 
                         alpha=0.6, color='green', label='å®‰å…¨', s=20)
        axes[1,1].scatter(crushed_data['current_force'], [1]*len(crushed_data), 
                         alpha=0.6, color='red', label='ã¤ã¶ã‚ŒãŸ', s=20)
        axes[1,1].plot(force_range, probabilities, 'b-', linewidth=2, label='AIäºˆæ¸¬æ›²ç·š')
        axes[1,1].set_xlabel(labels['force'])
        axes[1,1].set_ylabel('å®Ÿéš›ã®çµæœ / äºˆæ¸¬ç¢ºç‡')
        axes[1,1].legend()
        axes[1,1].grid(True, alpha=0.3)
        
        # 6. è¨“ç·´å±¥æ­´ï¼ˆDNNã®å ´åˆï¼‰
        if hasattr(self, 'training_history'):
            history = self.training_history.history
            axes[1,2].plot(history['loss'], label='Training Loss')
            axes[1,2].plot(history['val_loss'], label='Validation Loss')
            axes[1,2].set_xlabel('Epoch')
            axes[1,2].set_ylabel('Loss')
            axes[1,2].set_title(labels['training_history'])
            axes[1,2].legend()
            axes[1,2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def generate_ai_report(self):
        """AIåˆ†æçµæœã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\n" + "="*60)
        print("           ğŸ¤– AIæ‰‹æ³•ã«ã‚ˆã‚‹åŠ›å¢ƒç•Œåˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*60)
        
        # ãƒ‡ãƒ¼ã‚¿æ¦‚è¦
        print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
        print(f"  - ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(self.data)}")
        print(f"  - ç‰¹å¾´é‡æ•°: {len(self.get_feature_columns())}")
        print(f"  - ã¤ã¶ã‚ŒãŸäº‹ä¾‹: {len(self.data[self.data['is_crushed']])} ä»¶")
        print(f"  - å®‰å…¨äº‹ä¾‹: {len(self.data[~self.data['is_crushed']])} ä»¶")
        
        # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½
        if self.best_model is not None:
            best_name = None
            for name, model in self.models.items():
                if model == self.best_model:
                    best_name = name
                    break
            
            print(f"\nğŸ¥‡ æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_name.upper()}")
            print(f"  - AUC Score: {self.model_scores[best_name]['auc']:.4f}")
            print(f"  - Accuracy: {self.model_scores[best_name]['accuracy']:.4f}")
        
        # AIäºˆæ¸¬ã«ã‚ˆã‚‹æœ€é©ç¯„å›²
        safe_range, optimal_range = self.find_optimal_force_range()
        
        # ãƒªã‚¹ã‚¯è©•ä¾¡ä¾‹
        print(f"\nâš ï¸  ãƒªã‚¹ã‚¯è©•ä¾¡ä¾‹:")
        test_forces = np.linspace(self.data['current_force'].min(), 
                                self.data['current_force'].max(), 5)
        for force in test_forces:
            prob = self.predict_crush_probability(force)
            risk_level = "ä½" if prob < 0.05 else "ä¸­" if prob < 0.2 else "é«˜"
            print(f"  - {force:.2f} N: ã¤ã¶ã‚Œã‚‹ç¢ºç‡ {prob*100:.1f}% (ãƒªã‚¹ã‚¯: {risk_level})")
        
        print("\n" + "="*60)


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    csv_file = "force_boundary_classification_20250812_002330.csv"
    
    # AIåˆ†é¡å™¨ã‚’åˆæœŸåŒ–
    ai_classifier = AIForceBoundaryClassifier(csv_file)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†
    if ai_classifier.load_and_preprocess_data():
        print("âœ… ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿å®Œäº†")
        
        # å…¨AIãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´
        print("\nğŸš€ AIè¨“ç·´é–‹å§‹...")
        ai_classifier.train_all_models()
        
        # çµæœå¯è¦–åŒ–
        ai_classifier.visualize_ai_results()
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        ai_classifier.generate_ai_report()
        
        # å€‹åˆ¥äºˆæ¸¬ãƒ†ã‚¹ãƒˆ
        test_force = 20.0
        prob = ai_classifier.predict_crush_probability(test_force)
        print(f"\nğŸ§ª ãƒ†ã‚¹ãƒˆ: {test_force} Nã§ã¤ã¶ã‚Œã‚‹ç¢ºç‡ã¯ {prob*100:.1f}%ã§ã™")