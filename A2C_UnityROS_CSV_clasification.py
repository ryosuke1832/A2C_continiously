# grasp_force_a2c_tf2_with_csv.py
import socket
import json
import threading
import time
import numpy as np
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt
from collections import deque
from datetime import datetime

# TensorFlow 2.xè¨­å®š
tf.config.run_functions_eagerly(False)

class GraspForceA2CAgent:
    """æŠŠæŒåŠ›åˆ¶å¾¡ç”¨A2Cã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆTensorFlow 2.xå¯¾å¿œï¼‰"""
    
    def __init__(self, state_size=4, action_size=1, 
                 actor_lr=0.001, critic_lr=0.001, gamma=0.99):
        
        self.state_size = state_size  # [current_force, accumulated_force, is_crushed, time_step]
        self.action_size = action_size  # [target_force_adjustment]
        self.gamma = gamma
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰
        self.actor = self._build_actor()
        self.critic = self._build_critic()
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        self.actor_optimizer = tf.keras.optimizers.Adam(actor_lr)
        self.critic_optimizer = tf.keras.optimizers.Adam(critic_lr)
        
        # çµŒé¨“ãƒãƒƒãƒ•ã‚¡
        self.memory = deque(maxlen=2000)
        
        # åˆ¶å¾¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.force_range = (0.0, 25.0)  # åˆ¶å¾¡ã™ã‚‹åŠ›ã®ç¯„å›²
        self.safe_force_range = (2.0, 15.0)  # å®‰å…¨ãªåŠ›ã®ç¯„å›²
        self.episode_start_time = None  # ãƒªã‚»ãƒƒãƒˆæ™‚ã«è¨­å®šã•ã‚Œã‚‹ã‚ˆã†ã«å¤‰æ›´
            
        print("ğŸ¤– æŠŠæŒåŠ›åˆ¶å¾¡A2Cã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†ï¼ˆTensorFlow 2.xï¼‰")
    
    def _build_actor(self):
        """ã‚¢ã‚¯ã‚¿ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰"""
        model = tf.keras.Sequential([
            tf.keras.layers.Dense(64, activation='relu', input_shape=(self.state_size,)),
            tf.keras.layers.Dense(32, activation='relu'),
            tf.keras.layers.Dense(1, activation='tanh')  # åŠ›ã®èª¿æ•´å€¤: -1.0 ~ +1.0
        ], name='actor')
        
        return model
    
    def _build_critic(self):
        """ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰"""
        model = tf.keras.Sequential([
            tf.keras.layers.Dense(64, activation='relu', input_shape=(self.state_size,)),
            tf.keras.layers.Dense(32, activation='relu'),
            tf.keras.layers.Dense(1, activation=None)  # çŠ¶æ…‹ä¾¡å€¤
        ], name='critic')
        
        return model
    
    def get_action(self, state, exploration=True):
        """çŠ¶æ…‹ã‹ã‚‰è¡Œå‹•ã‚’æ±ºå®š"""
        state = tf.expand_dims(tf.convert_to_tensor(state, dtype=tf.float32), 0)
        
        # ã‚¢ã‚¯ã‚¿ãƒ¼ã‹ã‚‰åŠ›èª¿æ•´å€¤ã‚’å–å¾—
        force_adjustment = float(self.actor(state)[0][0].numpy())
        
        # æ¢ç´¢ãƒã‚¤ã‚ºè¿½åŠ 
        if exploration:
            noise = np.random.normal(0, 0.1)
            force_adjustment += noise
            force_adjustment = float(np.clip(force_adjustment, -1.0, 1.0))
        
        return force_adjustment
    
    def get_value(self, state):
        """çŠ¶æ…‹ä¾¡å€¤ã‚’å–å¾—"""
        state = tf.expand_dims(tf.convert_to_tensor(state, dtype=tf.float32), 0)
        value = float(self.critic(state)[0][0].numpy())
        return value
    
    def remember(self, state, action, reward, next_state, done):
        """çµŒé¨“ã‚’è¨˜æ†¶"""
        self.memory.append({
            'state': state,
            'action': action,
            'reward': reward,
            'next_state': next_state,
            'done': done
        })
    
    @tf.function
    def train_step(self, states, actions, rewards, next_states, dones):
        """å˜ä¸€ã®è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—"""
        with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:
            # ç¾åœ¨ã®çŠ¶æ…‹ä¾¡å€¤ã¨æ¬¡ã®çŠ¶æ…‹ä¾¡å€¤ã‚’è¨ˆç®—
            current_values = self.critic(states)
            next_values = self.critic(next_states)
            
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¾¡å€¤è¨ˆç®—ï¼ˆTDå­¦ç¿’ï¼‰
            target_values = rewards + self.gamma * next_values * (1 - dones)
            advantages = target_values - current_values
            
            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯æå¤±
            critic_loss = tf.reduce_mean(tf.square(advantages))
            
            # ã‚¢ã‚¯ã‚¿ãƒ¼æå¤±ï¼ˆãƒãƒªã‚·ãƒ¼å‹¾é…ï¼‰
            action_probs = self.actor(states)
            log_probs = -0.5 * tf.square((actions - action_probs) / 0.1)
            actor_loss = -tf.reduce_mean(log_probs * tf.stop_gradient(advantages))
        
        # å‹¾é…è¨ˆç®—ã¨é©ç”¨
        actor_gradients = actor_tape.gradient(actor_loss, self.actor.trainable_variables)
        critic_gradients = critic_tape.gradient(critic_loss, self.critic.trainable_variables)
        
        self.actor_optimizer.apply_gradients(zip(actor_gradients, self.actor.trainable_variables))
        self.critic_optimizer.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))
        
        return actor_loss, critic_loss
    
    def train(self, batch_size=32):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è¨“ç·´"""
        if len(self.memory) < batch_size:
            return None, None
        
        # ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        batch_indices = np.random.choice(len(self.memory), batch_size, replace=False)
        batch = [self.memory[i] for i in batch_indices]
        
        states = tf.convert_to_tensor([exp['state'] for exp in batch], dtype=tf.float32)
        actions = tf.convert_to_tensor([[exp['action']] for exp in batch], dtype=tf.float32)
        rewards = tf.convert_to_tensor([[exp['reward']] for exp in batch], dtype=tf.float32)
        next_states = tf.convert_to_tensor([exp['next_state'] for exp in batch], dtype=tf.float32)
        dones = tf.convert_to_tensor([[float(exp['done'])] for exp in batch], dtype=tf.float32)
        
        # è¨“ç·´å®Ÿè¡Œ
        actor_loss, critic_loss = self.train_step(states, actions, rewards, next_states, dones)
        
        return float(actor_loss.numpy()), float(critic_loss.numpy())

class GraspForceA2CServer:
    """Unityé€šä¿¡å¯¾å¿œã®æŠŠæŒåŠ›åˆ¶å¾¡A2Cã‚µãƒ¼ãƒãƒ¼ï¼ˆCSVä¿å­˜æ©Ÿèƒ½ä»˜ãï¼‰"""
    
    def __init__(self, host='127.0.0.1', port=12345):
        self.host = host
        self.port = port
        self.socket = None
        self.client_socket = None
        self.running = False
        
        # A2Cã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        self.agent = GraspForceA2CAgent()
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç®¡ç†
        self.current_episode = 0
        self.episode_rewards = []
        self.episode_steps = 0
        self.episode_start_time = time.time()
        
        # çŠ¶æ…‹ç®¡ç†
        self.current_state = None
        self.last_action = 0.0
        self.last_recommended_force = 10.0  # åˆæœŸæ¨å¥¨åŠ›
        
        # çµ±è¨ˆ
        self.total_episodes = 0
        self.successful_episodes = 0
        self.current_episode_reward = 0.0
        
        # CSVä¿å­˜ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        self.results_df = pd.DataFrame(columns=['game', 'steps', 'actor_cost', 'critic_cost', 'final_reward', 'action'])
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿
        self.episode_actions = []
        self.episode_actor_costs = []
        self.episode_critic_costs = []
        
        print("ğŸš€ æŠŠæŒåŠ›åˆ¶å¾¡A2Cã‚µãƒ¼ãƒãƒ¼åˆæœŸåŒ–å®Œäº†ï¼ˆCSVä¿å­˜æ©Ÿèƒ½ä»˜ãï¼‰")
    
    def start_server(self):
        """ã‚µãƒ¼ãƒãƒ¼é–‹å§‹"""
        try:
            self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            self.socket.settimeout(1.0)
            self.socket.bind((self.host, self.port))
            self.socket.listen(1)
            
            print("=" * 60)
            print("ğŸ¤– æŠŠæŒåŠ›åˆ¶å¾¡A2Cã‚µãƒ¼ãƒãƒ¼é–‹å§‹")
            print(f"ğŸ“¡ æ¥ç¶šå…ˆ: {self.host}:{self.port}")
            print("ğŸ¯ ç›®æ¨™: ã‚¢ãƒ«ãƒŸç¼¶ã‚’ã¤ã¶ã•ãªã„æœ€é©ãªæŠŠæŒåŠ›å­¦ç¿’")
            print("ğŸ’¾ çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™")
            print("=" * 60)
            
            self.running = True
            
            while self.running:
                try:
                    self.client_socket, addr = self.socket.accept()
                    print(f"âœ… Unityæ¥ç¶š: {addr}")
                    print(f"â° é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%H:%M:%S')}")
                    
                    self.communication_loop()
                    break
                    
                except socket.timeout:
                    continue
                except Exception as e:
                    if self.running:
                        print(f"âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
                    break
        
        except Exception as e:
            print(f"âŒ ã‚µãƒ¼ãƒãƒ¼é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            self.cleanup()
    
        
    def communication_loop(self):
        """ãƒ¡ã‚¤ãƒ³é€šä¿¡ãƒ«ãƒ¼ãƒ—"""
        print("ğŸ§  A2Cå­¦ç¿’ãƒ«ãƒ¼ãƒ—é–‹å§‹")
        print("ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºä¸­...")
        print()
        
        if self.client_socket:
            self.client_socket.settimeout(1.0)
        
        while self.running:
            try:
                data = self.client_socket.recv(4096)
                if not data:
                    break
                
                # ğŸ”¥ è¿½åŠ ï¼šå—ä¿¡ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¡¨ç¤º
                raw_message = data.decode('utf-8')
                print(f"ğŸ“¥ å—ä¿¡ãƒ‡ãƒ¼ã‚¿ï¼ˆç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰: {raw_message}")
                
                message = json.loads(raw_message)
                
                # ğŸ”¥ è¿½åŠ ï¼šJSONãƒ‘ãƒ¼ã‚¹å¾Œã®å†…å®¹ã‚‚è¡¨ç¤º
                print(f"ğŸ“‹ ãƒ‘ãƒ¼ã‚¹å¾Œã®å†…å®¹: {json.dumps(message, indent=2, ensure_ascii=False)}")
                
                response = self.process_a2c_message(message)
                
                if response:
                    response_json = json.dumps(response)
                    # ğŸ”¥ è¿½åŠ ï¼šé€ä¿¡ã™ã‚‹ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ã‚‚è¡¨ç¤º
                    print(f"ğŸ“¤ é€ä¿¡ãƒ‡ãƒ¼ã‚¿: {response_json}")
                    self.client_socket.send(response_json.encode('utf-8'))
                
            except socket.timeout:
                continue
            except json.JSONDecodeError as e:
                print(f"âŒ JSONError: {e}")
                print(f"ğŸ” ã‚¨ãƒ©ãƒ¼ã®åŸå› ãƒ‡ãƒ¼ã‚¿: {data.decode('utf-8') if data else 'None'}")  # ğŸ”¥ è¿½åŠ 
            except Exception as e:
                if self.running:
                    print(f"âŒ Communication Error: {e}")
                    print(f"ğŸ” ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ‡ãƒ¼ã‚¿: {data.decode('utf-8') if data else 'None'}")  # ğŸ”¥ è¿½åŠ 
                break
    
    def process_a2c_message(self, message):
        """A2Cå­¦ç¿’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†"""
        msg_type = message.get('type')
        
        # ğŸ”¥ è¿½åŠ ï¼šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—ã¨åŸºæœ¬æƒ…å ±ã‚’è¡¨ç¤º
        print(f"ğŸ¯ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†é–‹å§‹: ã‚¿ã‚¤ãƒ—={msg_type}, ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—={message.get('timestamp', 'ãªã—')}")
        
        if msg_type == 'can_state':
            # ğŸ”¥ è¿½åŠ ï¼šç¼¶ã®çŠ¶æ…‹è©³ç´°ã‚’è¡¨ç¤º
            current_force = message.get('current_force', 0.0)
            accumulated_force = message.get('accumulated_force', 0.0)
            is_crushed = message.get('is_crushed', False)
            print(f"ğŸ¥¤ ç¼¶çŠ¶æ…‹: ç¾åœ¨åŠ›={current_force:.2f}N, ç´¯ç©åŠ›={accumulated_force:.2f}N, ã¤ã¶ã‚Œ={is_crushed}")
            
            return self.handle_can_state_learning(message)
        elif msg_type == 'episode_end':
            print(f"ğŸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡")
            return self.handle_episode_end(message)
        elif msg_type == 'reset':
            print(f"ğŸ”„ ãƒªã‚»ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡")
            return self.handle_reset(message)
        elif msg_type == 'ping':
            print(f"ğŸ“ Pingãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡")
            return {
                'type': 'pong', 
                'message': 'A2Cã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå‹•ä½œä¸­',
                'timestamp': float(time.time())
            }
        else:
            # ğŸ”¥ è¿½åŠ ï¼šä¸æ˜ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—ã®è©³ç´°è¡¨ç¤º
            print(f"â“ ä¸æ˜ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—: {msg_type}")
            print(f"ğŸ” ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¨ä½“: {json.dumps(message, indent=2, ensure_ascii=False)}")
        
        return None

    def communication_loop(self):
        print("ğŸ§  A2Cå­¦ç¿’ãƒ«ãƒ¼ãƒ—é–‹å§‹")
        print("ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºä¸­...")
        print()
        
        if self.client_socket:
            self.client_socket.settimeout(1.0)
        
        while self.running:
            try:
                data = self.client_socket.recv(4096)
                if not data:
                    break
                
                # ğŸ”¥ è¿½åŠ ï¼šå—ä¿¡ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾è¡¨ç¤º
                raw_message = data.decode('utf-8')
                print(f"ğŸ“¥ å—ä¿¡ãƒ‡ãƒ¼ã‚¿ï¼ˆç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰: {raw_message}")
                
                message = json.loads(raw_message)
                
                # ğŸ”¥ è¿½åŠ ï¼šJSONãƒ‘ãƒ¼ã‚¹å¾Œã®å†…å®¹ã‚‚è¡¨ç¤º
                print(f"ğŸ“‹ ãƒ‘ãƒ¼ã‚¹å¾Œã®å†…å®¹: {json.dumps(message, indent=2, ensure_ascii=False)}")
                
                response = self.process_a2c_message(message)
                
                if response:
                    response_json = json.dumps(response)
                    # ğŸ”¥ è¿½åŠ ï¼šé€ä¿¡ã™ã‚‹ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ã‚‚è¡¨ç¤º
                    print(f"ğŸ“¤ é€ä¿¡ãƒ‡ãƒ¼ã‚¿: {response_json}")
                    self.client_socket.send(response_json.encode('utf-8'))
                
            except socket.timeout:
                continue
            except json.JSONDecodeError as e:
                print(f"âŒ JSONError: {e}")
                print(f"ğŸ” ã‚¨ãƒ©ãƒ¼ã®åŸå› ãƒ‡ãƒ¼ã‚¿: {data.decode('utf-8') if data else 'None'}")  # ğŸ”¥ è¿½åŠ 
            except Exception as e:
                if self.running:
                    print(f"âŒ Communication Error: {e}")
                    print(f"ğŸ” ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ‡ãƒ¼ã‚¿: {data.decode('utf-8') if data else 'None'}")  # ğŸ”¥ è¿½åŠ 
                break

    def process_a2c_message(self, message):
        """A2Cå­¦ç¿’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†"""
        msg_type = message.get('type')
        
        # ğŸ”¥ è¿½åŠ ï¼šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—ã¨åŸºæœ¬æƒ…å ±ã‚’è¡¨ç¤º
        print(f"ğŸ¯ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†é–‹å§‹: ã‚¿ã‚¤ãƒ—={msg_type}, ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—={message.get('timestamp', 'ãªã—')}")
        
        if msg_type == 'can_state':
            # ğŸ”¥ è¿½åŠ ï¼šç¼¶ã®çŠ¶æ…‹è©³ç´°ã‚’è¡¨ç¤º
            current_force = message.get('current_force', 0.0)
            accumulated_force = message.get('accumulated_force', 0.0)
            is_crushed = message.get('is_crushed', False)
            print(f"ğŸ¥¤ ç¼¶çŠ¶æ…‹: ç¾åœ¨åŠ›={current_force:.2f}N, ç´¯ç©åŠ›={accumulated_force:.2f}N, ã¤ã¶ã‚Œ={is_crushed}")
            
            return self.handle_can_state_learning(message)
        elif msg_type == 'episode_end':
            print(f"ğŸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡")
            return self.handle_episode_end(message)
        elif msg_type == 'reset':
            print(f"ğŸ”„ ãƒªã‚»ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡")
            return self.handle_reset(message)
        elif msg_type == 'ping':
            print(f"ğŸ“ Pingãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡")
            return {
                'type': 'pong', 
                'message': 'A2Cã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå‹•ä½œä¸­',
                'timestamp': float(time.time())
            }
        else:
            # ğŸ”¥ è¿½åŠ ï¼šä¸æ˜ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—ã®è©³ç´°è¡¨ç¤º
            print(f"â“ ä¸æ˜ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—: {msg_type}")
            print(f"ğŸ” ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¨ä½“: {json.dumps(message, indent=2, ensure_ascii=False)}")
        
        return None
    
        
    def handle_can_state_learning(self, message):
        """æŠŠæŒçŠ¶æ…‹ã§ã®å­¦ç¿’å‡¦ç†"""
        # ğŸ”¥ è¿½åŠ ï¼šå—ä¿¡ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è©³ç´°ãƒ­ã‚°
        print(f"ğŸ“Š å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å—ä¿¡:")
        for key, value in message.items():
            print(f"   {key}: {value}")
        
        # ç¾åœ¨ã®çŠ¶æ…‹ã‚’æ§‹ç¯‰
        current_force = message.get('current_force', 0.0)
        accumulated_force = message.get('accumulated_force', 0.0)
        is_crushed = message.get('is_crushed', False)
        timestamp = message.get('timestamp', time.time())
        
        # ğŸ”¥ ä¿®æ­£ï¼šã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–‹å§‹æ™‚é–“ã®åˆæœŸåŒ–ãƒã‚§ãƒƒã‚¯
        if self.episode_start_time is None:
            self.episode_start_time = timestamp
            print(f"âš ï¸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–‹å§‹æ™‚é–“ãŒæœªè¨­å®šã ã£ãŸãŸã‚ã€ç¾åœ¨æ™‚é–“ã§åˆæœŸåŒ–: {timestamp:.3f}ç§’")
        
        # ğŸ”¥ ä¿®æ­£ï¼šæ™‚é–“æ­£è¦åŒ–ã®è¨ˆç®—ã‚’æ”¹å–„
        elapsed_time = timestamp - self.episode_start_time
        time_step = elapsed_time / 30.0  # 30ç§’ã§æ­£è¦åŒ–
        
        # ğŸ”¥ è¿½åŠ ï¼šæ™‚é–“è¨ˆç®—ã®ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
        print(f"â° æ™‚é–“è¨ˆç®—: ç¾åœ¨æ™‚é–“={timestamp:.3f}, é–‹å§‹æ™‚é–“={self.episode_start_time:.3f}, çµŒéæ™‚é–“={elapsed_time:.3f}, æ­£è¦åŒ–å€¤={time_step:.6f}")
        
        # æ­£è¦åŒ–ã•ã‚ŒãŸçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«
        new_state = [
            current_force / 25.0,  # 25Nã§æ­£è¦åŒ–
            accumulated_force / 25.0,
            float(is_crushed),
            min(max(time_step, 0.0), 1.0)  # ğŸ”¥ ä¿®æ­£ï¼š0-1ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—
        ]
        
        # ğŸ”¥ ä¿®æ­£ï¼šæ­£è¦åŒ–å¾Œã®çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ã‚‚è¡¨ç¤º
        print(f"ğŸ§® æ­£è¦åŒ–çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«: {new_state}")
        
        # å ±é…¬è¨ˆç®—
        reward = self.calculate_grasp_reward(current_force, accumulated_force, is_crushed)
        self.current_episode_reward += reward
        
        # ğŸ”¥ è¿½åŠ ï¼šå ±é…¬è¨ˆç®—ã®è©³ç´°
        print(f"ğŸ’° è¨ˆç®—ã•ã‚ŒãŸå ±é…¬: {reward:.3f}, ç´¯ç©å ±é…¬: {self.current_episode_reward:.3f}")
        
        
        # æå¤±å€¤ã®åˆæœŸåŒ–
        actor_loss, critic_loss = 0.0, 0.0
        
        # å­¦ç¿’å‡¦ç†ï¼ˆå‰ã®çŠ¶æ…‹ãŒã‚ã‚‹å ´åˆï¼‰
        if self.current_state is not None:
            done = is_crushed  # ã¤ã¶ã‚ŒãŸã‚‰ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†
            
            # çµŒé¨“ã‚’ä¿å­˜
            self.agent.remember(
                self.current_state, 
                self.last_action, 
                reward, 
                new_state, 
                done
            )
            
            # å­¦ç¿’å®Ÿè¡Œ
            if len(self.agent.memory) > 32:
                actor_loss, critic_loss = self.agent.train()
                
                # æå¤±å€¤ã‚’è¨˜éŒ²ï¼ˆå­¦ç¿’ãŒå®Ÿè¡Œã•ã‚ŒãŸå ´åˆã®ã¿ï¼‰
                if actor_loss is not None and critic_loss is not None:
                    self.episode_actor_costs.append(actor_loss)
                    self.episode_critic_costs.append(critic_loss)
                
                if actor_loss is not None and self.episode_steps % 10 == 0:  # 10ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«è¡¨ç¤º
                    print(f"ğŸ“ˆ Episode {self.current_episode}, Step {self.episode_steps}: "
                          f"å ±é…¬={reward:.2f}, "
                          f"ç¾åœ¨åŠ›={current_force:.2f}N, "
                          f"Actoræå¤±={actor_loss:.4f}, "
                          f"Criticæå¤±={critic_loss:.4f}")
        
        # æ¬¡ã®è¡Œå‹•ã‚’æ±ºå®š
        action = self.agent.get_action(new_state, exploration=True)
        
        # è¡Œå‹•ã‚’æ¨å¥¨åŠ›ã«å¤‰æ›
        recommended_force = self.action_to_force(action, current_force)
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨˜éŒ²
        self.episode_actions.append(recommended_force)
        
        # çŠ¶æ…‹æ›´æ–°
        self.current_state = new_state
        self.last_action = action
        self.last_recommended_force = recommended_force
        self.episode_steps += 1
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ãƒã‚§ãƒƒã‚¯
        if is_crushed or self.episode_steps > 100:  # ã¤ã¶ã‚Œã‚‹ã‹100ã‚¹ãƒ†ãƒƒãƒ—ã§çµ‚äº†
            self.end_current_episode(is_crushed)
        
        return {
            'type': 'a2c_action',
            'recommended_force': float(recommended_force),
            'current_reward': float(reward),
            'episode': int(self.current_episode),
            'step': int(self.episode_steps),
            'exploration_factor': float(action),
            'timestamp': float(time.time())
        }
    
    def calculate_grasp_reward(self, current_force, accumulated_force, is_crushed):
        """æŠŠæŒå ±é…¬è¨ˆç®—"""
        if is_crushed:
            return -100.0  # ã¤ã¶ã‚ŒãŸã‚‰å¤§ããªãƒšãƒŠãƒ«ãƒ†ã‚£
        
        # åŸºæœ¬å ±é…¬ï¼šé©åˆ‡ãªåŠ›ç¯„å›²ã«ã„ã‚‹ã“ã¨ã«å¯¾ã™ã‚‹å ±é…¬
        if 2.0 <= current_force <= 15.0:
            # æœ€é©ç¯„å›²ã§ã®å ±é…¬ï¼ˆä¸­å¤®ä»˜è¿‘ã§æœ€å¤§ï¼‰
            optimal_force = 8.5  # æœ€é©åŠ›
            distance_from_optimal = abs(current_force - optimal_force)
            force_reward = 10.0 * (1.0 - distance_from_optimal / 6.5)  # æœ€å¤§10ãƒã‚¤ãƒ³ãƒˆ
        elif current_force < 2.0:
            # åŠ›ä¸è¶³ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
            force_reward = -5.0
        else:  # current_force > 15.0
            # éåº¦ãªåŠ›ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
            excess_force = current_force - 15.0
            force_reward = -excess_force * 2.0  # éå‰°åŠ›ã«æ¯”ä¾‹ã—ãŸãƒšãƒŠãƒ«ãƒ†ã‚£
        
        # ç¶™ç¶šå ±é…¬ï¼šã¤ã¶ã•ãšã«ç¶­æŒã§ãã¦ã„ã‚‹æ™‚é–“ã«å¯¾ã™ã‚‹å ±é…¬
        time_reward = 1.0  # å„ã‚¹ãƒ†ãƒƒãƒ—ã§1ãƒã‚¤ãƒ³ãƒˆ
        
        # è“„ç©åŠ›ã«å¯¾ã™ã‚‹æ³¨æ„å–šèµ·
        if accumulated_force > 12.0:
            accumulation_penalty = -(accumulated_force - 12.0) * 0.5
        else:
            accumulation_penalty = 0.0
        
        total_reward = force_reward + time_reward + accumulation_penalty
        
        return total_reward
    
    def action_to_force(self, action, current_force):
        """è¡Œå‹•å€¤ã‚’æ¨å¥¨åŠ›ã«å¤‰æ›"""
        # actionã¯-1.0~1.0ã®ç¯„å›²
        # ç¾åœ¨ã®åŠ›ã‹ã‚‰Â±3Nã®ç¯„å›²ã§èª¿æ•´
        force_adjustment = float(action) * 3.0  # æœ€å¤§Â±3Nã®èª¿æ•´
        new_force = float(current_force) + force_adjustment
        
        # å®‰å…¨ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—
        new_force = float(np.clip(new_force, 0.0, 20.0))
        
        return new_force
    
    def end_current_episode(self, was_crushed):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å‡¦ç†ã¨CSVä¿å­˜"""
        self.total_episodes += 1
        self.episode_rewards.append(self.current_episode_reward)
        
        if not was_crushed:
            self.successful_episodes += 1
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®çµ±è¨ˆå€¤è¨ˆç®—
        avg_action = np.mean(self.episode_actions) if self.episode_actions else 0.0
        avg_actor_cost = np.mean(self.episode_actor_costs) if self.episode_actor_costs else 0.0
        avg_critic_cost = np.mean(self.episode_critic_costs) if self.episode_critic_costs else 0.0
        
        # CSVãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
        episode_data = {
            'game': self.current_episode,
            'steps': self.episode_steps,
            'actor_cost': avg_actor_cost,
            'critic_cost': avg_critic_cost,
            'final_reward': self.current_episode_reward,
            'action': avg_action
        }
        
        # DataFrameã«è¡Œã‚’è¿½åŠ 
        new_row = pd.DataFrame([episode_data])
        self.results_df = pd.concat([self.results_df, new_row], ignore_index=True)
        
        # é€²è¡Œçµ±è¨ˆã®è¨ˆç®—
        success_rate = (self.successful_episodes / self.total_episodes) * 100
        avg_reward = np.mean(self.episode_rewards[-10:]) if self.episode_rewards else 0
        
        print(f"\nğŸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {self.current_episode} çµ‚äº†")
        print(f"   çµæœ: {'âœ…æˆåŠŸ' if not was_crushed else 'âŒå¤±æ•—ï¼ˆã¤ã¶ã‚ŒãŸï¼‰'}")
        print(f"   ã‚¹ãƒ†ãƒƒãƒ—æ•°: {self.episode_steps}")
        print(f"   ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å ±é…¬: {self.current_episode_reward:.2f}")
        print(f"   å¹³å‡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {avg_action:.2f}N")
        print(f"   å¹³å‡Actoræå¤±: {avg_actor_cost:.4f}")
        print(f"   å¹³å‡Criticæå¤±: {avg_critic_cost:.4f}")
        print(f"   æˆåŠŸç‡: {success_rate:.1f}% ({self.successful_episodes}/{self.total_episodes})")
        print(f"   å¹³å‡å ±é…¬(ç›´è¿‘10å›): {avg_reward:.2f}")
        print("-" * 50)
        
        # å®šæœŸçš„ã«CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        if self.current_episode % 10 == 0:
            self.save_results_csv()
        
        # æ¬¡ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æº–å‚™
        self.current_episode += 1
        self.episode_steps = 0
        self.episode_start_time = time.time()
        self.current_state = None
        self.current_episode_reward = 0.0
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
        self.episode_actions = []
        self.episode_actor_costs = []
        self.episode_critic_costs = []
    
    def save_results_csv(self):
        """çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f'grasp_a2c_results_{timestamp}.csv'
        
        try:
            self.results_df.to_csv(filename, index=False)
            print(f"ğŸ’¾ çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {filename}")
        except Exception as e:
            print(f"âŒ CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def save_final_results(self):
        """æœ€çµ‚çµæœã¨ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜"""
        if len(self.results_df) == 0:
            print("âš ï¸ ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # æœ€çµ‚CSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_filename = f'grasp_a2c_final_results_{timestamp}.csv'
        
        try:
            self.results_df.to_csv(csv_filename, index=False)
            print(f"ğŸ’¾ æœ€çµ‚çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {csv_filename}")
            
            # å­¦ç¿’æ›²ç·šã®å¯è¦–åŒ–
            self.plot_learning_curves(timestamp)
            
        except Exception as e:
            print(f"âŒ æœ€çµ‚çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def plot_learning_curves(self, timestamp):
        """å­¦ç¿’æ›²ç·šã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            # ç§»å‹•å¹³å‡ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
            window_size = min(10, len(self.results_df))
            
            if window_size > 0:
                # ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®ç§»å‹•å¹³å‡
                steps_ma = self.results_df['steps'].rolling(window=window_size).mean()
                axes[0,0].plot(self.results_df['game'], steps_ma)
                axes[0,0].set_title('Steps per Episode (Moving Average)')
                axes[0,0].set_xlabel('Episode')
                axes[0,0].set_ylabel('Steps')
                axes[0,0].grid(True)
                
                # å ±é…¬ã®ç§»å‹•å¹³å‡
                reward_ma = self.results_df['final_reward'].rolling(window=window_size).mean()
                axes[0,1].plot(self.results_df['game'], reward_ma)
                axes[0,1].set_title('Final Reward per Episode (Moving Average)')
                axes[0,1].set_xlabel('Episode')
                axes[0,1].set_ylabel('Final Reward')
                axes[0,1].grid(True)
                
                # Actoræå¤±ã®ç§»å‹•å¹³å‡
                actor_cost_ma = self.results_df['actor_cost'].rolling(window=window_size).mean()
                axes[1,0].plot(self.results_df['game'], actor_cost_ma)
                axes[1,0].set_title('Actor Cost (Moving Average)')
                axes[1,0].set_xlabel('Episode')
                axes[1,0].set_ylabel('Actor Cost')
                axes[1,0].grid(True)
                
                # å¹³å‡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ç§»å‹•å¹³å‡
                action_ma = self.results_df['action'].rolling(window=window_size).mean()
                axes[1,1].plot(self.results_df['game'], action_ma)
                axes[1,1].set_title('Average Action per Episode (Moving Average)')
                axes[1,1].set_xlabel('Episode')
                axes[1,1].set_ylabel('Average Grip Force (N)')
                axes[1,1].grid(True)
            
            plt.tight_layout()
            plot_filename = f'grasp_a2c_learning_curves_{timestamp}.png'
            plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š å­¦ç¿’æ›²ç·šã‚’ä¿å­˜ã—ã¾ã—ãŸ: {plot_filename}")
            
            # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
            self.print_final_statistics()
            
        except Exception as e:
            print(f"âŒ ã‚°ãƒ©ãƒ•ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def print_final_statistics(self):
        """æœ€çµ‚çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
        if len(self.results_df) > 0:
            print("\n" + "=" * 60)
            print("ğŸ“Š æœ€çµ‚å­¦ç¿’çµ±è¨ˆ")
            print("-" * 60)
            print(f"ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {len(self.results_df)}")
            print(f"å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ•°: {self.results_df['steps'].mean():.2f}")
            print(f"å¹³å‡æœ€çµ‚å ±é…¬: {self.results_df['final_reward'].mean():.2f}")
            print(f"å¹³å‡Actoræå¤±: {self.results_df['actor_cost'].mean():.4f}")
            print(f"å¹³å‡Criticæå¤±: {self.results_df['critic_cost'].mean():.4f}")
            print(f"å¹³å‡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {self.results_df['action'].mean():.2f}N")
            
            # ç›´è¿‘10ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®çµ±è¨ˆ
            if len(self.results_df) >= 10:
                recent_data = self.results_df.tail(10)
                print(f"\nç›´è¿‘10ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®çµ±è¨ˆ:")
                print(f"å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ•°: {recent_data['steps'].mean():.2f}")
                print(f"å¹³å‡æœ€çµ‚å ±é…¬: {recent_data['final_reward'].mean():.2f}")
                print(f"å¹³å‡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {recent_data['action'].mean():.2f}N")
            
            print("=" * 60)
    
    def handle_episode_end(self, message):
        """æ˜ç¤ºçš„ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å‡¦ç†"""
        if self.current_state is not None:
            self.end_current_episode(False)  # æ­£å¸¸çµ‚äº†
        
        return {
            'type': 'episode_ack',
            'total_episodes': int(self.total_episodes),
            'success_rate': float((self.successful_episodes / max(1, self.total_episodes)) * 100),
            'timestamp': float(time.time())
        }
        
    def handle_reset(self, message):
        """ãƒªã‚»ãƒƒãƒˆå‡¦ç†"""
        self.current_state = None
        self.episode_steps = 0
        
        # ğŸ”¥ ä¿®æ­£ï¼šUnityæ™‚é–“ã«åˆã‚ã›ãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–‹å§‹æ™‚é–“ã®è¨­å®š
        unity_timestamp = message.get('timestamp', time.time())
        self.episode_start_time = unity_timestamp  # Unityæ™‚é–“ã‚’ä½¿ç”¨
        
        # ğŸ”¥ è¿½åŠ ï¼šãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
        print(f"ğŸ”„ ãƒªã‚»ãƒƒãƒˆå‡¦ç†ï¼šUnityæ™‚é–“={unity_timestamp:.3f}ç§’ã‚’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–‹å§‹æ™‚é–“ã¨ã—ã¦è¨­å®š")
        
        self.current_episode_reward = 0.0
        self.episode_actions = []
        self.episode_actor_costs = []
        self.episode_critic_costs = []
        
        return {
            'type': 'reset_ack',  
            'message': 'A2Cã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆå®Œäº†',
            'timestamp': unity_timestamp 
        }
    
    def cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        self.running = False
        
        # æœ€çµ‚çµæœã‚’ä¿å­˜
        self.save_final_results()
        
        if self.client_socket:
            self.client_socket.close()
        if self.socket:
            self.socket.close()
        
        # æœ€çµ‚çµ±è¨ˆè¡¨ç¤º
        print("\n" + "=" * 60)
        print("ğŸ† A2Cå­¦ç¿’çµ±è¨ˆ")
        print("-" * 60)
        print(f"ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {self.total_episodes}")
        print(f"æˆåŠŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {self.successful_episodes}")
        if self.total_episodes > 0:
            print(f"æœ€çµ‚æˆåŠŸç‡: {(self.successful_episodes / self.total_episodes) * 100:.2f}%")
            if self.episode_rewards:
                print(f"å¹³å‡å ±é…¬: {np.mean(self.episode_rewards):.2f}")
                print(f"æœ€é«˜å ±é…¬: {max(self.episode_rewards):.2f}")
        print("=" * 60)
        print("ğŸ›‘ A2Cã‚µãƒ¼ãƒãƒ¼åœæ­¢")

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
if __name__ == "__main__":
    print("æŠŠæŒåŠ›åˆ¶å¾¡A2Cã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆTensorFlow 2.xå¯¾å¿œãƒ»CSVä¿å­˜æ©Ÿèƒ½ä»˜ãï¼‰")
    print("Unityé€£æºã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    print()
    
    server = GraspForceA2CServer()
    try:
        server.start_server()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æ‰‹å‹•åœæ­¢ä¸­...")
        server.cleanup()