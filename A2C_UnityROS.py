# grasp_force_a2c_tf2.py
import socket
import json
import threading
import time
import numpy as np
import tensorflow as tf
from collections import deque
from datetime import datetime

# TensorFlow 2.xè¨­å®š
tf.config.run_functions_eagerly(False)

class GraspForceA2CAgent:
    """æŠŠæŒåŠ›åˆ¶å¾¡ç”¨A2Cã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆTensorFlow 2.xå¯¾å¿œï¼‰"""
    
    def __init__(self, state_size=4, action_size=1, 
                 actor_lr=0.001, critic_lr=0.001, gamma=0.99):
        
        self.state_size = state_size  # [current_force, accumulated_force, is_crushed, time_step]
        self.action_size = action_size  # [target_force_adjustment]
        self.gamma = gamma
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰
        self.actor = self._build_actor()
        self.critic = self._build_critic()
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
        self.actor_optimizer = tf.keras.optimizers.Adam(actor_lr)
        self.critic_optimizer = tf.keras.optimizers.Adam(critic_lr)
        
        # çµŒé¨“ãƒãƒƒãƒ•ã‚¡
        self.memory = deque(maxlen=2000)
        
        # åˆ¶å¾¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.force_range = (0.0, 25.0)  # åˆ¶å¾¡ã™ã‚‹åŠ›ã®ç¯„å›²
        self.safe_force_range = (2.0, 15.0)  # å®‰å…¨ãªåŠ›ã®ç¯„å›²
        
        print("ğŸ¤– æŠŠæŒåŠ›åˆ¶å¾¡A2Cã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†ï¼ˆTensorFlow 2.xï¼‰")
    
    def _build_actor(self):
        """ã‚¢ã‚¯ã‚¿ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰"""
        model = tf.keras.Sequential([
            tf.keras.layers.Dense(64, activation='relu', input_shape=(self.state_size,)),
            tf.keras.layers.Dense(32, activation='relu'),
            tf.keras.layers.Dense(1, activation='tanh')  # åŠ›ã®èª¿æ•´å€¤: -1.0 ~ +1.0
        ], name='actor')
        
        return model
    
    def _build_critic(self):
        """ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰"""
        model = tf.keras.Sequential([
            tf.keras.layers.Dense(64, activation='relu', input_shape=(self.state_size,)),
            tf.keras.layers.Dense(32, activation='relu'),
            tf.keras.layers.Dense(1, activation=None)  # çŠ¶æ…‹ä¾¡å€¤
        ], name='critic')
        
        return model
    
    def get_action(self, state, exploration=True):
        """çŠ¶æ…‹ã‹ã‚‰è¡Œå‹•ã‚’æ±ºå®š"""
        state = tf.expand_dims(tf.convert_to_tensor(state, dtype=tf.float32), 0)
        
        # ã‚¢ã‚¯ã‚¿ãƒ¼ã‹ã‚‰åŠ›èª¿æ•´å€¤ã‚’å–å¾—
        force_adjustment = float(self.actor(state)[0][0].numpy())
        
        # æ¢ç´¢ãƒã‚¤ã‚ºè¿½åŠ 
        if exploration:
            noise = np.random.normal(0, 0.1)
            force_adjustment += noise
            force_adjustment = float(np.clip(force_adjustment, -1.0, 1.0))
        
        return force_adjustment
    
    def get_value(self, state):
        """çŠ¶æ…‹ä¾¡å€¤ã‚’å–å¾—"""
        state = tf.expand_dims(tf.convert_to_tensor(state, dtype=tf.float32), 0)
        value = float(self.critic(state)[0][0].numpy())
        return value
    
    def remember(self, state, action, reward, next_state, done):
        """çµŒé¨“ã‚’è¨˜æ†¶"""
        self.memory.append({
            'state': state,
            'action': action,
            'reward': reward,
            'next_state': next_state,
            'done': done
        })
    
    @tf.function
    def train_step(self, states, actions, rewards, next_states, dones):
        """å˜ä¸€ã®è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—"""
        with tf.GradientTape() as actor_tape, tf.GradientTape() as critic_tape:
            # ç¾åœ¨ã®çŠ¶æ…‹ä¾¡å€¤ã¨æ¬¡ã®çŠ¶æ…‹ä¾¡å€¤ã‚’è¨ˆç®—
            current_values = self.critic(states)
            next_values = self.critic(next_states)
            
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¾¡å€¤è¨ˆç®—ï¼ˆTDå­¦ç¿’ï¼‰
            target_values = rewards + self.gamma * next_values * (1 - dones)
            advantages = target_values - current_values
            
            # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯æå¤±
            critic_loss = tf.reduce_mean(tf.square(advantages))
            
            # ã‚¢ã‚¯ã‚¿ãƒ¼æå¤±ï¼ˆãƒãƒªã‚·ãƒ¼å‹¾é…ï¼‰
            action_probs = self.actor(states)
            log_probs = -0.5 * tf.square((actions - action_probs) / 0.1)
            actor_loss = -tf.reduce_mean(log_probs * tf.stop_gradient(advantages))
        
        # å‹¾é…è¨ˆç®—ã¨é©ç”¨
        actor_gradients = actor_tape.gradient(actor_loss, self.actor.trainable_variables)
        critic_gradients = critic_tape.gradient(critic_loss, self.critic.trainable_variables)
        
        self.actor_optimizer.apply_gradients(zip(actor_gradients, self.actor.trainable_variables))
        self.critic_optimizer.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))
        
        return actor_loss, critic_loss
    
    def train(self, batch_size=32):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è¨“ç·´"""
        if len(self.memory) < batch_size:
            return None, None
        
        # ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        batch_indices = np.random.choice(len(self.memory), batch_size, replace=False)
        batch = [self.memory[i] for i in batch_indices]
        
        states = tf.convert_to_tensor([exp['state'] for exp in batch], dtype=tf.float32)
        actions = tf.convert_to_tensor([[exp['action']] for exp in batch], dtype=tf.float32)
        rewards = tf.convert_to_tensor([[exp['reward']] for exp in batch], dtype=tf.float32)
        next_states = tf.convert_to_tensor([exp['next_state'] for exp in batch], dtype=tf.float32)
        dones = tf.convert_to_tensor([[float(exp['done'])] for exp in batch], dtype=tf.float32)
        
        # è¨“ç·´å®Ÿè¡Œ
        actor_loss, critic_loss = self.train_step(states, actions, rewards, next_states, dones)
        
        return float(actor_loss.numpy()), float(critic_loss.numpy())

class GraspForceA2CServer:
    """Unityé€šä¿¡å¯¾å¿œã®æŠŠæŒåŠ›åˆ¶å¾¡A2Cã‚µãƒ¼ãƒãƒ¼"""
    
    def __init__(self, host='127.0.0.1', port=12345):
        self.host = host
        self.port = port
        self.socket = None
        self.client_socket = None
        self.running = False
        
        # A2Cã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        self.agent = GraspForceA2CAgent()
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç®¡ç†
        self.current_episode = 0
        self.episode_rewards = []
        self.episode_steps = 0
        self.episode_start_time = time.time()
        
        # çŠ¶æ…‹ç®¡ç†
        self.current_state = None
        self.last_action = 0.0
        self.last_recommended_force = 10.0  # åˆæœŸæ¨å¥¨åŠ›
        
        # çµ±è¨ˆ
        self.total_episodes = 0
        self.successful_episodes = 0
        self.current_episode_reward = 0.0
        
        print("ğŸš€ æŠŠæŒåŠ›åˆ¶å¾¡A2Cã‚µãƒ¼ãƒãƒ¼åˆæœŸåŒ–å®Œäº†")
    
    def start_server(self):
        """ã‚µãƒ¼ãƒãƒ¼é–‹å§‹"""
        try:
            self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            self.socket.settimeout(1.0)
            self.socket.bind((self.host, self.port))
            self.socket.listen(1)
            
            print("=" * 60)
            print("ğŸ¤– æŠŠæŒåŠ›åˆ¶å¾¡A2Cã‚µãƒ¼ãƒãƒ¼é–‹å§‹")
            print(f"ğŸ“¡ æ¥ç¶šå…ˆ: {self.host}:{self.port}")
            print("ğŸ¯ ç›®æ¨™: ã‚¢ãƒ«ãƒŸç¼¶ã‚’ã¤ã¶ã•ãªã„æœ€é©ãªæŠŠæŒåŠ›å­¦ç¿’")
            print("=" * 60)
            
            self.running = True
            
            while self.running:
                try:
                    self.client_socket, addr = self.socket.accept()
                    print(f"âœ… Unityæ¥ç¶š: {addr}")
                    print(f"â° é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%H:%M:%S')}")
                    
                    self.communication_loop()
                    break
                    
                except socket.timeout:
                    continue
                except Exception as e:
                    if self.running:
                        print(f"âŒ æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
                    break
        
        except Exception as e:
            print(f"âŒ ã‚µãƒ¼ãƒãƒ¼é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            self.cleanup()
    
    def communication_loop(self):
        """ãƒ¡ã‚¤ãƒ³é€šä¿¡ãƒ«ãƒ¼ãƒ—"""
        print("ğŸ§  A2Cå­¦ç¿’ãƒ«ãƒ¼ãƒ—é–‹å§‹")
        print("ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºä¸­...")
        print()
        
        if self.client_socket:
            self.client_socket.settimeout(1.0)
        
        while self.running:
            try:
                data = self.client_socket.recv(4096)
                if not data:
                    break
                
                message = json.loads(data.decode('utf-8'))
                response = self.process_a2c_message(message)
                
                if response:
                    response_json = json.dumps(response)
                    self.client_socket.send(response_json.encode('utf-8'))
                
            except socket.timeout:
                continue
            except json.JSONDecodeError as e:
                print(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
            except Exception as e:
                if self.running:
                    print(f"âŒ é€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
                break
    
    def process_a2c_message(self, message):
        """A2Cå­¦ç¿’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†"""
        msg_type = message.get('type')
        
        if msg_type == 'can_state':
            return self.handle_can_state_learning(message)
        elif msg_type == 'episode_end':
            return self.handle_episode_end(message)
        elif msg_type == 'reset':
            return self.handle_reset(message)
        elif msg_type == 'ping':
            return {
                'type': 'pong', 
                'message': 'A2Cã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå‹•ä½œä¸­',
                'timestamp': float(time.time())
            }
        
        return None
    
    def handle_can_state_learning(self, message):
        """æŠŠæŒçŠ¶æ…‹ã§ã®å­¦ç¿’å‡¦ç†"""
        # ç¾åœ¨ã®çŠ¶æ…‹ã‚’æ§‹ç¯‰
        current_force = message.get('current_force', 0.0)
        accumulated_force = message.get('accumulated_force', 0.0)
        is_crushed = message.get('is_crushed', False)
        timestamp = message.get('timestamp', time.time())
        
        # æ­£è¦åŒ–ã•ã‚ŒãŸçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«
        time_step = (timestamp - self.episode_start_time) / 30.0  # 30ç§’ã§æ­£è¦åŒ–
        new_state = [
            current_force / 25.0,  # 25Nã§æ­£è¦åŒ–
            accumulated_force / 25.0,
            float(is_crushed),
            min(time_step, 1.0)
        ]
        
        # å ±é…¬è¨ˆç®—
        reward = self.calculate_grasp_reward(current_force, accumulated_force, is_crushed)
        self.current_episode_reward += reward
        
        # å­¦ç¿’å‡¦ç†ï¼ˆå‰ã®çŠ¶æ…‹ãŒã‚ã‚‹å ´åˆï¼‰
        if self.current_state is not None:
            done = is_crushed  # ã¤ã¶ã‚ŒãŸã‚‰ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†
            
            # çµŒé¨“ã‚’ä¿å­˜
            self.agent.remember(
                self.current_state, 
                self.last_action, 
                reward, 
                new_state, 
                done
            )
            
            # å­¦ç¿’å®Ÿè¡Œ
            if len(self.agent.memory) > 32:
                actor_loss, critic_loss = self.agent.train()
                
                if actor_loss is not None and self.episode_steps % 10 == 0:  # 10ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«è¡¨ç¤º
                    print(f"ğŸ“ˆ Episode {self.current_episode}, Step {self.episode_steps}: "
                          f"å ±é…¬={reward:.2f}, "
                          f"ç¾åœ¨åŠ›={current_force:.2f}N, "
                          f"Actoræå¤±={actor_loss:.4f}, "
                          f"Criticæå¤±={critic_loss:.4f}")
        
        # æ¬¡ã®è¡Œå‹•ã‚’æ±ºå®š
        action = self.agent.get_action(new_state, exploration=True)
        
        # è¡Œå‹•ã‚’æ¨å¥¨åŠ›ã«å¤‰æ›
        recommended_force = self.action_to_force(action, current_force)
        
        # çŠ¶æ…‹æ›´æ–°
        self.current_state = new_state
        self.last_action = action
        self.last_recommended_force = recommended_force
        self.episode_steps += 1
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ãƒã‚§ãƒƒã‚¯
        if is_crushed or self.episode_steps > 100:  # ã¤ã¶ã‚Œã‚‹ã‹100ã‚¹ãƒ†ãƒƒãƒ—ã§çµ‚äº†
            self.end_current_episode(is_crushed)
        
        return {
            'type': 'a2c_action',
            'recommended_force': float(recommended_force),
            'current_reward': float(reward),
            'episode': int(self.current_episode),
            'step': int(self.episode_steps),
            'exploration_factor': float(action),
            'timestamp': float(time.time())
        }
    
    def calculate_grasp_reward(self, current_force, accumulated_force, is_crushed):
        """æŠŠæŒå ±é…¬è¨ˆç®—"""
        if is_crushed:
            return -100.0  # ã¤ã¶ã‚ŒãŸã‚‰å¤§ããªãƒšãƒŠãƒ«ãƒ†ã‚£
        
        # åŸºæœ¬å ±é…¬ï¼šé©åˆ‡ãªåŠ›ç¯„å›²ã«ã„ã‚‹ã“ã¨ã«å¯¾ã™ã‚‹å ±é…¬
        if 2.0 <= current_force <= 15.0:
            # æœ€é©ç¯„å›²ã§ã®å ±é…¬ï¼ˆä¸­å¤®ä»˜è¿‘ã§æœ€å¤§ï¼‰
            optimal_force = 8.5  # æœ€é©åŠ›
            distance_from_optimal = abs(current_force - optimal_force)
            force_reward = 10.0 * (1.0 - distance_from_optimal / 6.5)  # æœ€å¤§10ãƒã‚¤ãƒ³ãƒˆ
        elif current_force < 2.0:
            # åŠ›ä¸è¶³ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
            force_reward = -5.0
        else:  # current_force > 15.0
            # éåº¦ãªåŠ›ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
            excess_force = current_force - 15.0
            force_reward = -excess_force * 2.0  # éå‰°åŠ›ã«æ¯”ä¾‹ã—ãŸãƒšãƒŠãƒ«ãƒ†ã‚£
        
        # ç¶™ç¶šå ±é…¬ï¼šã¤ã¶ã•ãšã«ç¶­æŒã§ãã¦ã„ã‚‹æ™‚é–“ã«å¯¾ã™ã‚‹å ±é…¬
        time_reward = 1.0  # å„ã‚¹ãƒ†ãƒƒãƒ—ã§1ãƒã‚¤ãƒ³ãƒˆ
        
        # è“„ç©åŠ›ã«å¯¾ã™ã‚‹æ³¨æ„å–šèµ·
        if accumulated_force > 12.0:
            accumulation_penalty = -(accumulated_force - 12.0) * 0.5
        else:
            accumulation_penalty = 0.0
        
        total_reward = force_reward + time_reward + accumulation_penalty
        
        return total_reward
    
    def action_to_force(self, action, current_force):
        """è¡Œå‹•å€¤ã‚’æ¨å¥¨åŠ›ã«å¤‰æ›"""
        # actionã¯-1.0~1.0ã®ç¯„å›²
        # ç¾åœ¨ã®åŠ›ã‹ã‚‰Â±3Nã®ç¯„å›²ã§èª¿æ•´
        force_adjustment = float(action) * 3.0  # æœ€å¤§Â±3Nã®èª¿æ•´
        new_force = float(current_force) + force_adjustment
        
        # å®‰å…¨ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—
        new_force = float(np.clip(new_force, 0.0, 20.0))
        
        return new_force
    
    def end_current_episode(self, was_crushed):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å‡¦ç†"""
        self.total_episodes += 1
        self.episode_rewards.append(self.current_episode_reward)
        
        if not was_crushed:
            self.successful_episodes += 1
        
        success_rate = (self.successful_episodes / self.total_episodes) * 100
        avg_reward = np.mean(self.episode_rewards[-10:]) if self.episode_rewards else 0
        
        print(f"\nğŸ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {self.current_episode} çµ‚äº†")
        print(f"   çµæœ: {'âœ…æˆåŠŸ' if not was_crushed else 'âŒå¤±æ•—ï¼ˆã¤ã¶ã‚ŒãŸï¼‰'}")
        print(f"   ã‚¹ãƒ†ãƒƒãƒ—æ•°: {self.episode_steps}")
        print(f"   ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å ±é…¬: {self.current_episode_reward:.2f}")
        print(f"   æˆåŠŸç‡: {success_rate:.1f}% ({self.successful_episodes}/{self.total_episodes})")
        print(f"   å¹³å‡å ±é…¬(ç›´è¿‘10å›): {avg_reward:.2f}")
        print("-" * 50)
        
        # æ¬¡ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æº–å‚™
        self.current_episode += 1
        self.episode_steps = 0
        self.episode_start_time = time.time()
        self.current_state = None
        self.current_episode_reward = 0.0
    
    def handle_episode_end(self, message):
        """æ˜ç¤ºçš„ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†å‡¦ç†"""
        if self.current_state is not None:
            self.end_current_episode(False)  # æ­£å¸¸çµ‚äº†
        
        return {
            'type': 'episode_ack',
            'total_episodes': int(self.total_episodes),
            'success_rate': float((self.successful_episodes / max(1, self.total_episodes)) * 100),
            'timestamp': float(time.time())
        }
    
    def handle_reset(self, message):
        """ãƒªã‚»ãƒƒãƒˆå‡¦ç†"""
        self.current_state = None
        self.episode_steps = 0
        self.episode_start_time = time.time()
        self.current_episode_reward = 0.0
        
        return {
            'type': 'reset_ack',  
            'message': 'A2Cã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆå®Œäº†',
            'timestamp': float(time.time())
        }
    
    def cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        self.running = False
        
        if self.client_socket:
            self.client_socket.close()
        if self.socket:
            self.socket.close()
        
        # æœ€çµ‚çµ±è¨ˆè¡¨ç¤º
        print("\n" + "=" * 60)
        print("ğŸ† A2Cå­¦ç¿’çµ±è¨ˆ")
        print("-" * 60)
        print(f"ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {self.total_episodes}")
        print(f"æˆåŠŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {self.successful_episodes}")
        if self.total_episodes > 0:
            print(f"æœ€çµ‚æˆåŠŸç‡: {(self.successful_episodes / self.total_episodes) * 100:.2f}%")
            if self.episode_rewards:
                print(f"å¹³å‡å ±é…¬: {np.mean(self.episode_rewards):.2f}")
                print(f"æœ€é«˜å ±é…¬: {max(self.episode_rewards):.2f}")
        print("=" * 60)
        print("ğŸ›‘ A2Cã‚µãƒ¼ãƒãƒ¼åœæ­¢")

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
if __name__ == "__main__":
    print("æŠŠæŒåŠ›åˆ¶å¾¡A2Cã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆTensorFlow 2.xå¯¾å¿œï¼‰")
    print("Unityé€£æºã«ã‚ˆã‚‹å¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")
    print()
    
    server = GraspForceA2CServer()
    try:
        server.start_server()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æ‰‹å‹•åœæ­¢ä¸­...")
        server.cleanup()